# VerifyBot v2 -- LLM-Driven Hardware Debug Loop

## What This Is

A Python tool that automates the copy-paste debugging cycle between an LLM (ChatGPT) and real hardware (Raspberry Pi 5, local machine). Instead of you manually copying terminal errors, pasting them into ChatGPT, copying the fix, and running it -- VerifyBot does the whole loop automatically.

The LLM is the brain. VerifyBot is just hands on the keyboard.

## How It Works

```
python main.py "make a random word generator that saves to a text file"
```

1. **Probe**: SSH into Pi, run diagnostic commands (uname, python version, pip list, ls, etc.), save as `context/raspi.md`
2. **Prompt**: Inject system context + user prompt into ChatGPT via browser automation
3. **Extract**: Parse response for ```python and ```bash code blocks
4. **Execute**: Upload scripts to Pi via SFTP and run via SSH, or run bash commands directly
5. **Verify**: Check exit codes and output
6. **Retry**: If failed, send raw stdout/stderr back to ChatGPT in the same conversation. Loop until success or max retries.

## Directory Structure

```
verifybot/
├── main.py                    # Entry point -- the entire pipeline
├── core/
│   ├── __init__.py            # Package marker
│   ├── selectors.py           # ChatGPT DOM selectors (update when frontend changes)
│   └── session.py             # Persistent browser session (prompt, followup, new_chat)
├── skills/
│   ├── __init__.py            # Package marker
│   ├── chatgpt_skill.py       # Browser automation wrapper, raw_md saving, login mode
│   ├── ssh_skill.py           # SSH/SFTP to Raspberry Pi via paramiko
│   └── extract_skill.py       # Code block extraction and classification from LLM responses
├── context/                   # Auto-generated system context (live Pi/local snapshots)
│   ├── raspi.md               # Generated by probe_pi() at pipeline start
│   └── local.md               # Generated by probe_local() at pipeline start
├── programs/                  # Extracted scripts — versioned, never overwritten
│   ├── prime_gen_1.py         # Attempt 1
│   ├── prime_gen_2.py         # Attempt 2 (fixed version)
│   └── ...
├── outputs/                   # Execution outputs — versioned, never overwritten
│   ├── prime_gen_1.txt        # stdout/stderr from attempt 1
│   ├── prime_gen_2.txt        # stdout/stderr from attempt 2
│   └── ...
├── raw_md/                    # Pipeline run transcripts (timestamped logs)
├── .env                       # Pi SSH credentials (PI_USER, PI_HOST, PI_PASSWORD)
├── .browser_profile/          # Chromium cookies (persistent ChatGPT login)
├── .gitignore
└── LLM.md                     # This file
```

## Import Map

```
main.py
  ├── core.session          (ChatGPTSession)
  ├── skills.chatgpt_skill  (save_response, append_to_log)
  ├── skills.ssh_skill      (ssh_run, ssh_run_detached, sftp_upload)
  └── skills.extract_skill  (extract_blocks, classify_blocks, extract_timeout_hint)

skills/chatgpt_skill.py
  ├── core.selectors
  └── core.session

core/session.py
  └── core.selectors

core/intents.py
  (terminal intent classification + prompt rule snippets)

skills/ssh_skill.py          (no internal imports, reads .env)
skills/extract_skill.py      (no internal imports, pure regex)
skills/terminal_skill.py     (terminal verification prompt helpers)
core/selectors.py            (no imports, just constants)
```

## Key Design Decisions

### The LLM is the brain, VerifyBot is the tool
VerifyBot does NOT try to diagnose errors, pick strategies, or add intelligence. It captures output faithfully and sends it back to the LLM. As LLMs improve, the tool gets better for free. Hardcoding heuristics is a losing game.

### LLM verifies output, not exit codes
After executing code, VerifyBot sends the full stdout/stderr back to ChatGPT and asks: "Does this correctly complete the task?" The LLM responds PASS, FAIL (with fix), or REVISE (with changes). This means even a program that exits 0 but produces wrong output gets caught. VerifyBot never decides success or failure — the LLM does.

### Versioned programs and outputs — never overwrite
Every attempt saves files with `_1`, `_2`, `_3` suffixes. `programs/prime_gen_1.py` is attempt 1, `programs/prime_gen_2.py` is the fix. `outputs/prime_gen_1.txt` has the stdout/stderr from attempt 1. You always have full history to see what changed between versions. Duplicate scripts (identical code across prompts) are automatically detected and skipped.

### Raw markdown logs include everything
The `raw_md/` transcript files embed saved scripts and execution outputs inline in chronological order. Each pipeline run produces a single `.md` file that contains: the prompts sent, LLM responses, saved script contents (with language-fenced code), and execution outputs. This means you can read one file and see the entire conversation + code + results without jumping between directories.

### Terminology: "Prompts" not "Attempts"
Each interaction with the LLM in a pipeline run is called a "Prompt" (Prompt 1, Prompt 2, etc.) in all logs and output files. Prompt 1 is the initial request, subsequent prompts are verification/retry cycles.

### Context injection at startup
Before the first prompt, VerifyBot SSHs into the Pi and gathers: hostname, kernel, Python version, pip packages, working directory contents, disk/memory, running processes, I2C/GPIO/serial state. This is saved to `context/raspi.md` and injected into the initial prompt. On each new chat, the probe runs again and picks up any changes the previous session made.

### LLM-predicted timeouts
The LLM is prompted to include `TIMEOUT: <seconds>` when it knows a script needs longer than the default 30s. VerifyBot reads this and adjusts the execution timeout. This solves the false-negative problem where long-running scripts get killed prematurely.

### No mode detection -- just execute what you get
The old system had separate "terminal loop" and "file pipeline" modes with regex-based routing. v2 removes this: the LLM responds with scripts, bash commands, or both. VerifyBot extracts and executes whatever it gets, in order. No upfront mode decision needed.

### Browser-based LLM, not API
Uses Playwright to automate ChatGPT's browser UI. No API keys, no per-token costs. The LLM layer is a clean interface (core/session.py) that could be swapped for any browser-based LLM. Response detection uses a stability check -- after the stop-generating button disappears, it waits for content to stabilize before extracting, preventing premature extraction of partial responses.

### Compilation happens on the target
C/C++ files are uploaded to the Pi (or kept local) and compiled there. No cross-compilation complexity. The target machine has the right toolchain for itself.

## Usage

```bash
# Primary interface rule (IMPORTANT)
# Use only this shape for day-to-day use:
#   python main.py "<your request>"
# Do NOT require users to run advanced dev-style commands,
# inline Python snippets, or multi-command validation invocations.

# First time: log in to ChatGPT manually
python main.py --login

# Basic usage (target auto-detected from prompt keywords)
python main.py "make a random word generator for raspi"
python main.py "write a fizzbuzz" --target local
python main.py "kill the infinite counter script"

# Terminal automation through the LLM loop
python main.py "i want to rollback my recent commit to b202dc86e85a9c6ec79a330782abde01f87df73e"
python main.py "check git status, then fetch origin and show me last 10 commits"

# Debugging hardware
python main.py "why is my I2C sensor not responding"
python main.py "set up CAN bus between Pi and STM32"

# Options
python main.py "stress test" --max-retries 5 --timeout 120
python main.py "blink LED" --headless
python main.py "read sensor" --remote-dir /home/scoobyxd/hw/pi

```

## Simplicity Rule for Commands

For user-facing instructions and examples, default to exactly this format:

```bash
python main.py "<natural language request>"
```

Avoid asking the user to run advanced/dev-style commands such as:
- heredocs (`python - <<'PY' ...`)
- wildcard/glob compilation checks (`core/*.py`, `skills/*.py`)
- ad-hoc inline validation scripts

Those are acceptable for internal development/testing by maintainers, but they are not the expected everyday user experience.

## CLI Flags

| Flag | Default | What it does |
|------|---------|-------------|
| `"prompt"` | (required) | Natural language task |
| `--target local/raspi` | auto-detect | Force execution target |
| `--headless` | OFF | Hide browser window |
| `--max-retries N` | 3 | Retry attempts on failure |
| `--timeout N` | 30 | Default execution timeout (LLM can override with TIMEOUT: hint) |
| `--remote-dir /path` | ~/Documents | Working directory on Pi |
| `--login` | — | Open browser for manual ChatGPT login |
## Terminal Tasks in the LLM Loop

Terminal tasks (including git workflows like rollback/revert/reset/push) are handled through the same v2 architecture:

1. You give one natural-language prompt.
2. VerifyBot asks the LLM for executable commands/scripts.
3. VerifyBot executes returned commands locally (or on Pi if target is raspi).
4. VerifyBot sends raw command/script output back to the LLM for verification.
5. The loop continues until `PASS` or retries are exhausted.

For terminal-intent prompts, VerifyBot now adds extra instruction pressure in the initial prompt so the LLM returns command-first, executable bash blocks suitable for immediate execution.

## Dependencies

```bash
pip install playwright paramiko
playwright install chromium
```

| Package | Purpose |
|---------|---------|
| playwright | Browser automation for ChatGPT |
| paramiko | SSH/SFTP to Raspberry Pi |

## Rules

1. ASCII only in all generated code and output. No emojis, no Unicode symbols.
2. SSH credentials live in `.env` (gitignored). Never hardcode.
3. All skill files use `_skill` suffix in their filename.
4. `__init__.py` files are required in `core/` and `skills/`. Do not delete.
5. Context files in `context/` are auto-generated. Do not manually edit.
6. The `programs/` directory keeps ALL versions. Files are named `slug_1.py`, `slug_2.py`, etc.
7. The `outputs/` directory keeps ALL execution outputs. Same versioning scheme.
8. `raw_md/` contains full pipeline transcripts for debugging VerifyBot itself.
8. **Dotfiles lose their leading dot when downloaded from Claude.ai.** After downloading, rename `gitignore` to `.gitignore` and `env` to `.env`. This is a browser download artifact, not a bug in the code.
9. **Pycache auto-cleanup.** `main.py` deletes all `__pycache__/` directories on startup. You do NOT need to manually delete them when updating files.
10. **Browser login persistence.** ChatGPT cookies are saved in `.browser_profile/`. Run `python main.py --login` once to log in manually. If the browser opens without being logged in, your `.browser_profile/` directory may have been deleted or moved -- run `--login` again. Do NOT delete `.browser_profile/` unless you want to re-login.

# VerifyBot v2 -- LLM-Driven Hardware Debug Loop

## What This Is

A Python tool that automates the copy-paste debugging cycle between an LLM (ChatGPT) and real hardware (Raspberry Pi 5, local machine). Instead of you manually copying terminal errors, pasting them into ChatGPT, copying the fix, and running it -- VerifyBot does the whole loop automatically.

The LLM is the brain. VerifyBot is just hands on the keyboard.

## How It Works

```
python main.py "make a random word generator that saves to a text file"
```

1. **Probe**: SSH into Pi, run diagnostic commands (uname, python version, pip list, ls, etc.), save as `context/raspi.md`
2. **Prompt**: Inject system context + user prompt into ChatGPT via browser automation
3. **Extract**: Parse response for ```python and ```bash code blocks
4. **Execute**: Upload scripts to Pi via SFTP and run via SSH, or run bash commands directly
5. **Verify**: Check exit codes and output
6. **Retry**: If failed, send raw stdout/stderr back to ChatGPT in the same conversation. Loop until success or max retries.

## Directory Structure

```
verifybot/
├── main.py                    # Entry point -- the entire pipeline
├── core/
│   ├── __init__.py            # Package marker
│   ├── selectors.py           # ChatGPT DOM selectors (update when frontend changes)
│   └── session.py             # Persistent browser session (prompt, followup, new_chat)
├── skills/
│   ├── __init__.py            # Package marker
│   ├── chatgpt_skill.py       # Browser automation wrapper, raw_md saving, login mode
│   ├── ssh_skill.py           # SSH/SFTP to Raspberry Pi via paramiko
│   └── extract_skill.py       # Code block extraction and classification from LLM responses
├── context/                   # Auto-generated system context (live Pi/local snapshots)
│   ├── raspi.md               # Generated by probe_pi() at pipeline start
│   └── local.md               # Generated by probe_local() at pipeline start
├── programs/                  # Extracted scripts — versioned, never overwritten
│   ├── prime_gen_1.py         # Attempt 1
│   ├── prime_gen_2.py         # Attempt 2 (fixed version)
│   └── ...
├── outputs/                   # Execution outputs — versioned, never overwritten
│   ├── prime_gen_1.txt        # stdout/stderr from attempt 1
│   ├── prime_gen_2.txt        # stdout/stderr from attempt 2
│   └── ...
├── raw_md/                    # Pipeline run transcripts (timestamped logs)
├── .env                       # Pi SSH credentials (PI_USER, PI_HOST, PI_PASSWORD)
├── .browser_profile/          # Chromium cookies (persistent ChatGPT login)
├── .gitignore
└── LLM.md                     # This file
```

## Import Map

```
main.py
  ├── core.session          (ChatGPTSession)
  ├── skills.chatgpt_skill  (save_response, append_to_log)
  ├── skills.ssh_skill      (ssh_run, ssh_run_live, ssh_run_detached, sftp_upload)
  └── skills.extract_skill  (extract_blocks, classify_blocks, extract_timeout_hint)

skills/chatgpt_skill.py
  ├── core.selectors
  └── core.session

core/session.py
  └── core.selectors

skills/ssh_skill.py          (no internal imports, reads .env)
skills/extract_skill.py      (no internal imports, pure regex)
core/selectors.py            (no imports, just constants)
```

## Key Design Decisions

### The LLM is the brain, VerifyBot is the tool
VerifyBot does NOT try to diagnose errors, pick strategies, or add intelligence. It captures output faithfully and sends it back to the LLM. As LLMs improve, the tool gets better for free. Hardcoding heuristics is a losing game.

### LLM verifies output, not exit codes
After executing code, VerifyBot sends the full stdout/stderr back to ChatGPT and asks: "Does this correctly complete the task?" The LLM responds PASS, FAIL (with fix), or REVISE (with changes). This means even a program that exits 0 but produces wrong output gets caught. VerifyBot never decides success or failure — the LLM does.

### Versioned programs and outputs — never overwrite
Every attempt saves files with `_1`, `_2`, `_3` suffixes. `programs/prime_gen_1.py` is attempt 1, `programs/prime_gen_2.py` is the fix. `outputs/prime_gen_1.txt` has the stdout/stderr from attempt 1. You always have full history to see what changed between versions. Duplicate scripts (identical code across prompts) are automatically detected and skipped.

### Raw markdown logs include everything
The `raw_md/` transcript files embed saved scripts and execution outputs inline in chronological order. Each pipeline run produces a single `.md` file that contains: the prompts sent, LLM responses, saved script contents (with language-fenced code), and execution outputs. This means you can read one file and see the entire conversation + code + results without jumping between directories.

### Terminology: "Prompts" not "Attempts"
Each interaction with the LLM in a pipeline run is called a "Prompt" (Prompt 1, Prompt 2, etc.) in all logs and output files. Prompt 1 is the initial request, subsequent prompts are verification/retry cycles.

### Context injection at startup
Before the first prompt, VerifyBot SSHs into the Pi and gathers: hostname, kernel, Python version, pip packages, working directory contents, disk/memory, running processes, I2C/GPIO/serial state. This is saved to `context/raspi.md` and injected into the initial prompt. On each new chat, the probe runs again and picks up any changes the previous session made. For local targets, context includes the git branch, bash availability, and installed compilers.

### Prompt engineering for reliable extraction
The initial prompt tells the LLM to put ALL code in exactly ONE fenced code block, and to put NO text after the closing fence. This prevents ChatGPT's usage instructions ("Save as...", "How to run...") from leaking into the extracted code. The prompt also stresses simplicity: fewer lines = fewer bugs.

### LLM-predicted timeouts
The LLM is prompted to include `TIMEOUT: <seconds>` when it knows a script needs longer than the default 30s. VerifyBot reads this and adjusts the execution timeout. This solves the false-negative problem where long-running scripts get killed prematurely.

### No mode detection -- just execute what you get
The old system had separate "terminal loop" and "file pipeline" modes with regex-based routing. v2 removes this: the LLM responds with scripts, bash commands, or both. VerifyBot extracts and executes whatever it gets, in order. No upfront mode decision needed.

### Local target = Python only
When target is `local`, the LLM is told to ALWAYS write Python. Even for simple tasks like creating folders, running git commands, or moving files -- the LLM uses `subprocess.run()`, `os`, `shutil`, and `pathlib` instead of bash. This eliminates all bash-on-Windows problems (WSL path mangling, Git Bash detection, cmd.exe incompatibilities). If the LLM writes a `.sh` script anyway, `run_script_local` returns a clear error telling it to rewrite in Python. The Raspberry Pi target still supports bash, Python, and C/C++ since it's native Linux over SSH.

### Browser-based LLM, not API
Uses Playwright to automate ChatGPT's browser UI. No API keys, no per-token costs. The LLM layer is a clean interface (core/session.py) that could be swapped for any browser-based LLM. Response detection uses a stability check -- after the stop-generating button disappears, it waits for content to stabilize before extracting, preventing premature extraction of partial responses.

### Live terminal streaming for Pi execution
When scripts or commands run on the Pi, output streams to your terminal in real-time via `ssh_run_live()`. You see every print statement, every error, every status message as it happens -- not just a captured dump after execution finishes. Output is formatted with box-drawing characters and color-coded (red for stderr). The original `ssh_run()` still exists for non-interactive use (probing, quick commands).

### CRLF normalization on save
Scripts extracted from ChatGPT's DOM on Windows may carry `\r\n` line endings. Before saving to `programs/`, all `\r` characters are stripped and files are written with explicit `\n` (Unix LF). This prevents `$'\r': command not found` errors when bash scripts are uploaded and run on the Pi.

### Compilation happens on the target
C/C++ files are uploaded to the Pi (or kept local) and compiled there. No cross-compilation complexity. The target machine has the right toolchain for itself.

## Usage

```bash
# First time: log in to ChatGPT manually
python main.py --login

# Basic usage (target auto-detected from prompt keywords)
python main.py "make a random word generator for raspi"
python main.py "write a fizzbuzz" --target local
python main.py "kill the infinite counter script"

# Debugging hardware
python main.py "why is my I2C sensor not responding"
python main.py "set up CAN bus between Pi and STM32"

# Options
python main.py "stress test" --max-retries 5 --timeout 120
python main.py "blink LED" --headless
python main.py "read sensor" --remote-dir /home/scoobyxd/hw/pi

# SSH skill standalone
python -m skills.ssh_skill --test
python -m skills.ssh_skill --run "ls -la ~/Documents"
```

## CLI Flags

| Flag | Default | What it does |
|------|---------|-------------|
| `"prompt"` | (required) | Natural language task |
| `--target local/raspi` | auto-detect | Force execution target |
| `--headless` | OFF | Hide browser window |
| `--max-retries N` | 3 | Retry attempts on failure |
| `--timeout N` | 30 | Default execution timeout (LLM can override with TIMEOUT: hint) |
| `--remote-dir /path` | ~/Documents | Working directory on Pi |
| `--login` | — | Open browser for manual ChatGPT login |

## Dependencies

```bash
pip install playwright paramiko
playwright install chromium
```

| Package | Purpose |
|---------|---------|
| playwright | Browser automation for ChatGPT |
| paramiko | SSH/SFTP to Raspberry Pi |

## Rules

1. ASCII only in all generated code and output. No emojis, no Unicode symbols.
2. SSH credentials live in `.env` (gitignored). Never hardcode.
3. All skill files use `_skill` suffix in their filename.
4. `__init__.py` files are required in `core/` and `skills/`. Do not delete.
5. Context files in `context/` are auto-generated. Do not manually edit.
6. The `programs/` directory keeps ALL versions. Files are named `slug_1.py`, `slug_2.py`, etc.
7. The `outputs/` directory keeps ALL execution outputs. Same versioning scheme.
8. `raw_md/` contains full pipeline transcripts for debugging VerifyBot itself.
8. **Dotfiles lose their leading dot when downloaded from Claude.ai.** After downloading, rename `gitignore` to `.gitignore` and `env` to `.env`. This is a browser download artifact, not a bug in the code.
9. **Pycache auto-cleanup.** `main.py` deletes all `__pycache__/` directories on startup. You do NOT need to manually delete them when updating files.
10. **Browser login persistence.** ChatGPT cookies are saved in `.browser_profile/`. Run `python main.py --login` once to log in manually. If the browser opens without being logged in, your `.browser_profile/` directory may have been deleted or moved -- run `--login` again. Do NOT delete `.browser_profile/` unless you want to re-login.
